{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bf2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "\n",
    "from utils.logging import *\n",
    "from data.process import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d38c17",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e98465a",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({\n",
    "    'checkpoint':'checkpoint/',\n",
    "    'version': '1.0',\n",
    "    'image_dir': 'Data/',\n",
    "    'patch_label': 'Metadata/PatchLabels.csv',\n",
    "    'predicting_var': 'response',\n",
    "    'prediction': 'binary classification', # ['regression', 'binary classification', classification']\n",
    "    'cohort': 'Cohort1',\n",
    "    'magnification': '10X',\n",
    "    'num_classes': 1,\n",
    "    'upsample': False,\n",
    "    'train_val_split': 0.7,\n",
    "    'random_crop': False,\n",
    "    'features': 2048,\n",
    "    'base_epoch': 19,\n",
    "    'batch_size': 256,\n",
    "    'workers': 4,\n",
    "    'seed': 0,\n",
    "    'gpu': 0\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162aa379",
   "metadata": {},
   "source": [
    "### Generating and saving features functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ddb3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_slides(slides, patch_labels, data_transforms, model):\n",
    "    for slide in slides:\n",
    "        slide_patch_labels = patch_labels[patch_labels.slide==slide].reset_index(drop=True)\n",
    "        dataset = PatchPathDataset(patch_labels=slide_patch_labels, image_folder=args.image_dir,\n",
    "                                           predicting_var=args.predicting_var, transform=data_transforms)\n",
    "        save_slide_features(slide, dataset, model)\n",
    "        \n",
    "\n",
    "def save_slide_features(slide, dataset, model):\n",
    "    # slide should be slide name\n",
    "    feature_path = os.path.join(args.checkpoint, f'BaselineResNet{args.version}', 'Features', \n",
    "                                f'epoch_{args.base_epoch}')\n",
    "    if not os.path.exists(feature_path):\n",
    "        os.makedirs(feature_path)\n",
    "    slide_feature_path = os.path.join(feature_path, slide)\n",
    "    if os.path.exists(slide_feature_path):\n",
    "        print(f'Features already exist for slide {slide} so won\\'t be generated again')\n",
    "    else:\n",
    "        print(f'Generating and saving features for slide {slide} at {slide_feature_path}')\n",
    "        slide_embeddings, patch_paths = generate_features(slide, dataset, model)\n",
    "        slide_embeddings_paths = {'slide_embeddings': slide_embeddings, 'patch_paths': patch_paths}\n",
    "        torch.save(slide_embeddings_paths, slide_feature_path)\n",
    "\n",
    "    \n",
    "def generate_features(slide, dataset, model):\n",
    "    model.eval()\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=args.batch_size, shuffle=False, \n",
    "                                         num_workers=args.workers, pin_memory=True, sampler=None)\n",
    "\n",
    "    slide_embeddings = []\n",
    "    patch_paths = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (images, _, paths) in enumerate(loader):\n",
    "            if args.gpu is not None:\n",
    "                images = images.cuda(args.gpu, non_blocking=True)\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            slide_embeddings += [outputs]\n",
    "            patch_paths += paths\n",
    "            del images, outputs, paths\n",
    "\n",
    "    slide_embeddings = torch.vstack(slide_embeddings)\n",
    "    return slide_embeddings, patch_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839d76bc",
   "metadata": {},
   "source": [
    "# Save feature embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85127ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "    \n",
    "    if not torch.cuda.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "    elif args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "\n",
    "    # Load baseline ResNet\n",
    "    print('Loading baseline ResNet')\n",
    "    base_model = torchvision.models.resnet50(pretrained=True)\n",
    "    base_model.fc = nn.Linear(in_features=args.features, out_features=args.num_classes, bias=True)\n",
    "    basenet_path = os.path.join(args.checkpoint, f'BaselineResNet{args.version}', f'epoch_{args.base_epoch}', \n",
    "                                'checkpoint.pth.tar')\n",
    "    print(f'Using baseline model: at {basenet_path}')\n",
    "    assert os.path.isfile(basenet_path)\n",
    "    resnet_state = torch.load(basenet_path, map_location=torch.device('cuda'))\n",
    "    resnet_state_dict = resnet_state['state_dict']\n",
    "    saved_val_cases = resnet_state['val_cases']\n",
    "    base_model.load_state_dict(resnet_state_dict, strict=True)\n",
    "    \n",
    "    feature_model = torch.nn.Sequential(*(list(base_model.children())[:-1]))\n",
    "    feature_model = feature_model.cuda(args.gpu)\n",
    "    del base_model\n",
    "    feature_model.eval()\n",
    "    \n",
    "    # Load data\n",
    "    patch_labels = pd.read_csv(args.patch_label, index_col=0)\n",
    "    patch_labels = patch_labels[patch_labels.magnification == args.magnification]\n",
    "    patch_labels = patch_labels.dropna(subset=[args.predicting_var])\n",
    "    patch_labels = select_cohort(patch_labels, args.cohort)\n",
    "\n",
    "    train_patch_labels, val_patch_labels, val_cases, _ = split_train_val(patch_labels, args.cohort, \n",
    "                                                                         args.train_val_split, args.seed, \n",
    "                                                                         args.prediction, args.predicting_var,\n",
    "                                                                         args.upsample)\n",
    "    # check saved_val_cases from baseline model are same as val_cases for attention model\n",
    "    assert (val_cases == saved_val_cases).all()\n",
    "    \n",
    "    train_slides = train_patch_labels.slide.unique()\n",
    "    print(f'{len(train_slides)} training slides')\n",
    "    val_slides = val_patch_labels.slide.unique()\n",
    "    print(f'{len(val_slides)} validation slides')\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "    \n",
    "    full_transforms, lim_transforms = image_transforms(args.random_crop)\n",
    "    \n",
    "    process_slides(train_slides, train_patch_labels, full_transforms, feature_model)\n",
    "    process_slides(val_slides, val_patch_labels, lim_transforms, feature_model)\n",
    "\n",
    "    del feature_model\n",
    "    print('Saved all features.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
