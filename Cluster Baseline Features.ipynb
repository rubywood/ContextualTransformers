{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d5962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import shutil\n",
    "import datetime\n",
    "from itertools import chain\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "\n",
    "from utils.logging import *\n",
    "from data.process import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec285a8",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc449bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = pd.Series({\n",
    "    'checkpoint':'checkpoint/',\n",
    "    'version': '1.0',\n",
    "    'image_dir': 'Data/',\n",
    "    'patch_label': 'Metadata/PatchLabels.csv',\n",
    "    'predicting_var': 'response',\n",
    "    'prediction': 'binary classification', # ['regression', 'binary classification', classification']\n",
    "    'cohort': 'Cohort1',\n",
    "    'magnification': '10X',\n",
    "    'upsample': False,\n",
    "    'train_val_split': 0.7,\n",
    "    'base_epoch': 19,\n",
    "    'normalize': True,\n",
    "    'n_clusters': 4,\n",
    "    'batch_size': 256,\n",
    "    'workers': 4,\n",
    "    'seed': 0,\n",
    "    'gpu': 0,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0d3f25",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ec9ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_flat_embeddings(slides, embeddings_path, normalize=False):\n",
    "    all_embeddings = []\n",
    "\n",
    "    for slide in slides:\n",
    "        slide_embeddings_paths = torch.load(os.path.join(embeddings_path, slide),\n",
    "                                            map_location=torch.device('cuda'))\n",
    "        slide_embeddings = slide_embeddings_paths['slide_embeddings']\n",
    "\n",
    "        if normalize:\n",
    "            slide_embeddings = F.normalize(slide_embeddings, p=2, dim=1)\n",
    "\n",
    "        all_embeddings.append(slide_embeddings)\n",
    "    flattened_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "    return flattened_embeddings\n",
    "\n",
    "def slide_clusters_dict(cluster_labels, slides, patch_labels):\n",
    "    # using reset index purely for chronological index in cluster labels\n",
    "    subset_patch_labels = patch_labels[patch_labels.slide.isin(slides)].reset_index(drop=True)\n",
    "    print(f'Have {len(cluster_labels)} cluster labels for {len(slides)} slides.')\n",
    "    slide_clusters = {}\n",
    "    for slide in slides:\n",
    "        idx = subset_patch_labels[subset_patch_labels.slide == slide].index\n",
    "        slide_clusters[slide] = cluster_labels[idx]\n",
    "    return slide_clusters\n",
    "\n",
    "def save_clusters(slide_clusters, save_folder, n_clusters, normalize=False):\n",
    "    if not os.path.exists(save_folder):\n",
    "        os.makedirs(save_folder)\n",
    "    norm_str = ''\n",
    "    if normalize:\n",
    "        norm_str = 'normalized_'\n",
    "    pickle.dump(slide_clusters, \n",
    "                open(os.path.join(save_folder, f'{n_clusters}_{norm_str}clusters.p'), 'wb'))\n",
    "    print(f'Clusters saved to {save_folder}.')\n",
    "\n",
    "def save_cluster_model(model, cluster_path, n_clusters, normalize=False):\n",
    "    norm_str = ''\n",
    "    if normalize:\n",
    "        norm_str = 'normalized_'\n",
    "    if not os.path.exists(os.path.join(cluster_path, 'models')):\n",
    "        os.makedirs(os.path.join(cluster_path, 'models'))\n",
    "    pickle.dump(model, open(os.path.join(cluster_path, 'models',\n",
    "                                          f'{norm_str}{n_clusters}_clusters_KMeans_model.p'), 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c11e68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_worker(gpu, args):\n",
    "    global best_acc1\n",
    "    args.gpu = gpu\n",
    "\n",
    "    if not torch.cuda.is_available():\n",
    "        print('using CPU, this will be slow')\n",
    "    elif args.gpu is not None:\n",
    "        print(\"Use GPU: {} for training\".format(args.gpu))\n",
    "        torch.cuda.set_device(args.gpu)\n",
    "    \n",
    "    # Load data\n",
    "    patch_labels = pd.read_csv(args.patch_label, index_col=0)\n",
    "    patch_labels = patch_labels[patch_labels.magnification == args.magnification]\n",
    "    patch_labels = patch_labels.dropna(subset=[args.predicting_var])\n",
    "\n",
    "    train_patch_labels, val_patch_labels, val_cases, _ = split_train_val(patch_labels, args.cohort, \n",
    "                                                                         args.train_val_split, args.seed, \n",
    "                                                                         args.prediction, args.predicting_var,\n",
    "                                                                         args.upsample)\n",
    "\n",
    "    train_slides = train_patch_labels.slide.unique()\n",
    "    print(f'{len(train_slides)} training slides')\n",
    "    val_slides = val_patch_labels.slide.unique()\n",
    "    print(f'{len(val_slides)} validation slides')\n",
    "\n",
    "    features_path = os.path.join(args.checkpoint, f'BaselineResNet{args.version}', 'Features', \n",
    "                                f'epoch_{args.base_epoch}')\n",
    "    \n",
    "    # Load train features and fit KMeans model\n",
    "    train_features = load_flat_embeddings(train_slides, features_path, normalize=args.normalize)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=args.n_clusters, random_state=0).fit(train_features.cpu().detach().numpy())\n",
    "    train_cluster_labels = kmeans.labels_\n",
    "    del train_features\n",
    "    slide_clusters = slide_clusters_dict(train_cluster_labels, train_slides, train_patch_labels)\n",
    "    \n",
    "    cluster_path = os.path.join(args.checkpoint, f'BaselineResNet{args.version}', 'Clusters', \n",
    "                                f'epoch_{args.base_epoch}')\n",
    "    save_clusters(slide_clusters, os.path.join(cluster_path, 'Train'), args.n_clusters, args.normalize)\n",
    "    print(f'Train clusters distribution: {np.unique(train_cluster_labels,return_counts=True)}')\n",
    "    \n",
    "    # Load validation features and apply KMeans model\n",
    "    val_features = load_flat_embeddings(val_slides, features_path, normalize=args.normalize)\n",
    "\n",
    "    val_cluster_labels = kmeans.predict(val_features.cpu().detach().numpy())\n",
    "    del val_features\n",
    "    val_slide_clusters = slide_clusters_dict(val_cluster_labels, val_slides, val_patch_labels)\n",
    "    \n",
    "    save_clusters(val_slide_clusters, os.path.join(cluster_path, 'Validation'), args.n_clusters, args.normalize)\n",
    "    print(f'Val clusters distribution: {np.unique(val_cluster_labels,return_counts=True)}')\n",
    "    \n",
    "    save_cluster_model(kmeans, cluster_path, args.n_clusters, args.normalize)\n",
    "    \n",
    "    return slide_clusters, val_slide_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1342b5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        cudnn.deterministic = True\n",
    "        warnings.warn('You have chosen to seed training. '\n",
    "                      'This will turn on the CUDNN deterministic setting, '\n",
    "                      'which can slow down your training considerably! '\n",
    "                      'You may see unexpected behavior when restarting '\n",
    "                      'from checkpoints.')\n",
    "\n",
    "    if args.gpu is not None:\n",
    "        warnings.warn('You have chosen a specific GPU. This will completely '\n",
    "                      'disable data parallelism.')\n",
    "\n",
    "    return main_worker(args.gpu, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3971e2bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_slide_clusters, val_slide_clusters = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8758f9e5",
   "metadata": {},
   "source": [
    "## Plot cluster distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9076d66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Training set cluster labels')\n",
    "plt.hist(list(chain.from_iterable(train_slide_clusters.values())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Validation set cluster labels')\n",
    "plt.hist(list(chain.from_iterable(val_slide_clusters.values())))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba75ecfc",
   "metadata": {},
   "source": [
    "## Explore cluster centres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a85a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cluster_model(cluster_path, n_clusters, normalize=False):\n",
    "    norm_str = ''\n",
    "    if normalize:\n",
    "        norm_str = 'normalized'\n",
    "    return pickle.load(open(os.path.join(cluster_path, 'models',\n",
    "                                          f'{norm_str}_{n_clusters}_clusters_KMeans_model.p'), 'rb'))\n",
    "\n",
    "cluster_path = os.path.join(args.checkpoint, f'BaselineResNet{args.version}', 'Clusters', \n",
    "                                f'epoch_{args.base_epoch}')\n",
    "cluster_model = load_cluster_model(cluster_path, n_clusters=args.n_clusters, normalize=args.normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab2a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
